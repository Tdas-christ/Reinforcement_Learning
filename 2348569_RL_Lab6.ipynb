{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxm8fPgtaJ+AAuEQXSkuN2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tdas-christ/Reinforcement_Learning/blob/main/2348569_RL_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBRUGTHt9Ze9",
        "outputId": "077bd3b9-a275-465c-aea9-ff244c614a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Environment:\n",
            "S F F F\n",
            "F H F H\n",
            "F F F F\n",
            "H F F G\n",
            "\n",
            "Collecting data...\n",
            "Computing policy...\n",
            "Testing policy:\n",
            "S F F F\n",
            "F H F H\n",
            "F F F F\n",
            "H F F G\n",
            "\n",
            "Action: down\n",
            "S F F F\n",
            "P H F H\n",
            "F F F F\n",
            "H F F G\n",
            "\n",
            "Action: down\n",
            "S F F F\n",
            "F H F H\n",
            "P F F F\n",
            "H F F G\n",
            "\n",
            "Action: right\n",
            "S F F F\n",
            "F H F H\n",
            "F P F F\n",
            "H F F G\n",
            "\n",
            "Action: down\n",
            "S F F F\n",
            "F H F H\n",
            "F F F F\n",
            "H P F G\n",
            "\n",
            "Action: right\n",
            "S F F F\n",
            "F H F H\n",
            "F F F F\n",
            "H F P G\n",
            "\n",
            "Action: right\n",
            "S F F F\n",
            "F H F H\n",
            "F F F F\n",
            "H F F P\n",
            "\n",
            "Episode finished!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Custom Grid Environment\n",
        "class CustomGridEnvironment:\n",
        "    def __init__(self, grid_size=4, holes=None, goal=None):\n",
        "        \"\"\"\n",
        "        Create a grid environment.\n",
        "        Args:\n",
        "            grid_size (int): Size of the grid (e.g., 4x4).\n",
        "            holes (list of tuples): Positions of holes (e.g., [(1, 1), (2, 3)]).\n",
        "            goal (tuple): Position of the goal (e.g., (3, 3)).\n",
        "        \"\"\"\n",
        "        self.grid_size = grid_size\n",
        "        self.holes = holes if holes else [(1, 1), (1, 3), (3, 0)]\n",
        "        self.goal = goal if goal else (3, 3)\n",
        "        self.start = (0, 0)\n",
        "        self.state = self.start\n",
        "        self.actions = ['up', 'down', 'left', 'right']\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to the start state.\"\"\"\n",
        "        self.state = self.start\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take a step in the environment.\n",
        "        Args:\n",
        "            action (str): One of 'up', 'down', 'left', or 'right'.\n",
        "        Returns:\n",
        "            next_state (tuple): The new state after taking the action.\n",
        "            reward (float): The reward received after the action.\n",
        "            done (bool): Whether the episode is finished.\n",
        "        \"\"\"\n",
        "        if self.done:\n",
        "            raise Exception(\"Episode has ended. Please reset the environment.\")\n",
        "\n",
        "        # Map actions to movements\n",
        "        moves = {\n",
        "            'up': (-1, 0),\n",
        "            'down': (1, 0),\n",
        "            'left': (0, -1),\n",
        "            'right': (0, 1)\n",
        "        }\n",
        "\n",
        "        # Calculate new state\n",
        "        row, col = self.state\n",
        "        move = moves.get(action, (0, 0))\n",
        "        new_row, new_col = row + move[0], col + move[1]\n",
        "\n",
        "        # Ensure new state is within bounds\n",
        "        if 0 <= new_row < self.grid_size and 0 <= new_col < self.grid_size:\n",
        "            next_state = (new_row, new_col)\n",
        "        else:\n",
        "            next_state = self.state  # Invalid move, stay in the same state\n",
        "\n",
        "        # Determine reward and terminal condition\n",
        "        if next_state in self.holes:\n",
        "            reward = -1  # Falling into a hole\n",
        "            self.done = True\n",
        "        elif next_state == self.goal:\n",
        "            reward = 1  # Reaching the goal\n",
        "            self.done = True\n",
        "        else:\n",
        "            reward = -0.1  # Small penalty for each step\n",
        "\n",
        "        self.state = next_state\n",
        "        return next_state, reward, self.done\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render the grid environment.\"\"\"\n",
        "        grid = np.full((self.grid_size, self.grid_size), 'F')  # F for Frozen tile\n",
        "        for hole in self.holes:\n",
        "            grid[hole] = 'H'  # H for Hole\n",
        "        grid[self.goal] = 'G'  # G for Goal\n",
        "        grid[self.state] = 'P'  # P for Player\n",
        "        grid[self.start] = 'S'  # S for Start\n",
        "        for row in grid:\n",
        "            print(\" \".join(row))\n",
        "        print()\n",
        "\n",
        "# Model-Based Agent\n",
        "class ModelBasedAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.states = [(i, j) for i in range(env.grid_size) for j in range(env.grid_size)]\n",
        "        self.actions = env.actions\n",
        "        self.transitions = {}  # Transition probabilities T(s, a, s')\n",
        "        self.rewards = {}  # Reward function R(s, a)\n",
        "        self.policy = {}  # Optimal policy Ï€(s)\n",
        "\n",
        "    def collect_data(self, num_episodes=100):\n",
        "        \"\"\"Collect transition and reward data.\"\"\"\n",
        "        for _ in range(num_episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = random.choice(self.actions)\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "                if (state, action) not in self.transitions:\n",
        "                    self.transitions[(state, action)] = {}\n",
        "                if next_state not in self.transitions[(state, action)]:\n",
        "                    self.transitions[(state, action)][next_state] = 0\n",
        "                self.transitions[(state, action)][next_state] += 1\n",
        "                self.rewards[(state, action)] = reward\n",
        "                state = next_state\n",
        "\n",
        "    def compute_policy(self, gamma=0.9, iterations=100):\n",
        "        \"\"\"Compute optimal policy using Value Iteration.\"\"\"\n",
        "        values = {s: 0 for s in self.states}  # Initialize values\n",
        "        for _ in range(iterations):\n",
        "            new_values = values.copy()\n",
        "            for s in self.states:\n",
        "                action_values = []\n",
        "                for a in self.actions:\n",
        "                    if (s, a) in self.transitions:\n",
        "                        q_value = 0\n",
        "                        total_transitions = sum(self.transitions[(s, a)].values())\n",
        "                        for s_next, count in self.transitions[(s, a)].items():\n",
        "                            prob = count / total_transitions\n",
        "                            reward = self.rewards.get((s, a), 0)\n",
        "                            q_value += prob * (reward + gamma * values[s_next])\n",
        "                        action_values.append(q_value)\n",
        "                if action_values:\n",
        "                    new_values[s] = max(action_values)\n",
        "            values = new_values\n",
        "\n",
        "        # Derive policy\n",
        "        for s in self.states:\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "            for a in self.actions:\n",
        "                if (s, a) in self.transitions:\n",
        "                    q_value = 0\n",
        "                    total_transitions = sum(self.transitions[(s, a)].values())\n",
        "                    for s_next, count in self.transitions[(s, a)].items():\n",
        "                        prob = count / total_transitions\n",
        "                        reward = self.rewards.get((s, a), 0)\n",
        "                        q_value += prob * (reward + gamma * values[s_next])\n",
        "                    if q_value > best_value:\n",
        "                        best_value = q_value\n",
        "                        best_action = a\n",
        "            self.policy[s] = best_action\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Choose an action based on the learned policy.\"\"\"\n",
        "        return self.policy.get(state, random.choice(self.actions))\n",
        "\n",
        "# Main Program\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the environment\n",
        "    env = CustomGridEnvironment(grid_size=4)\n",
        "\n",
        "    # Render the environment\n",
        "    print(\"Initial Environment:\")\n",
        "    env.render()\n",
        "\n",
        "    # Initialize the agent\n",
        "    agent = ModelBasedAgent(env)\n",
        "\n",
        "    # Collect data by interacting with the environment\n",
        "    print(\"Collecting data...\")\n",
        "    agent.collect_data(num_episodes=500)\n",
        "\n",
        "    # Compute the optimal policy\n",
        "    print(\"Computing policy...\")\n",
        "    agent.compute_policy()\n",
        "\n",
        "    # Test the policy\n",
        "    print(\"Testing policy:\")\n",
        "    state = env.reset()\n",
        "    env.render()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        print(f\"Action: {action}\")\n",
        "        state, reward, done = env.step(action)\n",
        "        env.render()\n",
        "    print(\"Episode finished!\")\n"
      ]
    }
  ]
}